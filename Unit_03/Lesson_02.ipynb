{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Gradient Descent Optimization in TensorFlow__\n",
    "Date : 20 Oct 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___`What is Gradient Descent?`___\n",
    "\n",
    "Gradient Descent is an iterative optimization method that updates model parameters to minimize a cost function. The cost function represents the difference between predicted and actual outcomes in a model, such as Mean Squared Error (MSE) in regression tasks.\n",
    "\n",
    "At each iteration, the parameters are updated by moving in the direction opposite to the gradient of the cost function. The size of each step is determined by the learning rate. The goal is to reach a local or global minimum where the gradient is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___`How Gradient Descent Works`___\n",
    "\n",
    "The key steps of gradient descent are:\n",
    "\n",
    "1. **Initialize Parameters**: Start with random parameter values.\n",
    "2. **Calculate Gradient**: Compute the gradient of the cost function with respect to the parameters.\n",
    "3. **Update Parameters**: Update the parameters by subtracting a fraction of the gradient (scaled by the learning rate).\n",
    "4. **Repeat**: Continue until convergence, i.e., until the cost function reaches a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Example:`__ Minimizing \\( f(x) = x^2 \\)\n",
    "1. Start with \\( x = 3 \\).\n",
    "2. The gradient(derivative) at \\( x = 3 \\) is \\( 2x = 6 \\).\n",
    "3. Update \\( x \\) using a learning rate of 0.1:  \n",
    "   \\( x = 3 - 0.1 \\times 6 = 2.4 \\).\n",
    "4. Repeat this process until the minimum is reached at \\( x = 0 \\).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
