{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Basics of Activation Functions__\n",
    "__Date :__ 5, Aug, 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put in simple terms, an artificial neuron calculates the ‘weighted sum’ of its inputs and adds a bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mathematically,__      \n",
    "    \n",
    "net input = ∑(weight * input)+bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the value of net input can be any anything from -infinity to +infinity. The neuron doesn’t really know how to bound to value and thus is not able to decide the firing pattern. Thus the activation function is an important part of an artificial neural network. They basically decide whether a neuron should be activated or not. Thus it bounds the value of the net input. The activation function is a non-linear transformation that we do over the input before sending it to the next layer of neurons or finalizing it as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Types of Activation Functions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Step Function:__ \n",
    "                    \n",
    "Step Function is one of the simplest kind of activation functions. In this, we consider a threshold value and if the value of net input say y is greater than the threshold then the neuron is activated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__MAthematically,__\n",
    "\n",
    " f(x) = 1, if x >= 0                \n",
    " f(x) = 0, if x < 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Sigmoid Function:__\n",
    "\n",
    "This is a smooth function and is continuously differentiable. The biggest advantage that it has over step and linear function is that it is non-linear. This is an incredibly cool feature of the sigmoid function. This essentially means that when I have multiple neurons having sigmoid function as their activation function – the output is non linear as well. The function ranges from 0-1 having an S shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Methematically,__\n",
    "\n",
    "1 / (1+e^-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. __ReLU:__ \n",
    "\n",
    "* The ReLU function is the Rectified linear unit. It is the most widely used activation function.\n",
    "\n",
    "* The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time. What does this mean ? If you look at the ReLU function if the input is negative it will convert it to zero and the neuron does not get activated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Methematically,__\n",
    "\n",
    " f(x) = max(0, x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. __Leaky ReLU:__ \n",
    "            \n",
    "Leaky ReLU function is nothing but an improved version of the ReLU function.Instead of defining the Relu function as 0 for x less than 0, we define it as a small linear component of x. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Methematically,__\n",
    "\n",
    "f(x) = ax, x<0              \n",
    "f(x) = x, otherwise"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
