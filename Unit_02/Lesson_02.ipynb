{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Types Of Activation Function in ANN__\n",
    "__Date :__ 10, Aug, 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The biological neural network has been modeled using Artificial Neural Networks (ANNs), where artificial neurons simulate the function of biological neurons. The artificial neuron is depicted in the image below.\n",
    "\n",
    "### Structure of an Artificial Neuron\n",
    "\n",
    "Each artificial neuron consists of three major components:\n",
    "\n",
    "1. **Synapses**: A set of 'i' synapses having weights \\( w_i \\). A signal \\( x_i \\) forms the input to the \\( i \\)-th synapse with weight \\( w_i \\). The value of any weight may be positive or negative. A positive weight has an excitatory effect, while a negative weight has an inhibitory effect on the output of the summation junction.\n",
    "\n",
    "2. **Summation Junction**: This is responsible for combining the input signals weighted by their respective synaptic weights. Since it acts as a linear combiner or adder of the weighted input signals, the output of the summation junction can be expressed as:\n",
    "\n",
    "   \\[\n",
    "   y_{\\text{sum}} = \\sum_{i=1}^{n} w_i x_i\n",
    "   \\]\n",
    "\n",
    "3. **Activation Function**: A threshold activation function (or simply the activation function, also known as a squashing function) results in an output signal only when the input signal exceeds a specific threshold value. This is similar to the behavior of a biological neuron, which transmits a signal only when the total input signal meets the firing threshold.\n",
    "\n",
    "### Types of Activation Functions\n",
    "\n",
    "There are different types of activation functions commonly used in artificial neurons. The most widely used ones are listed below:\n",
    "\n",
    "#### A. Identity Function\n",
    "\n",
    "The identity function is used as an activation function for the input layer. It is a linear function of the form:\n",
    "\n",
    "\\[\n",
    "y_{\\text{out}} = f(x) = x, \\quad \\forall x\n",
    "\\]\n",
    "\n",
    "As is obvious, the output remains the same as the input.\n",
    "\n",
    "#### B. Threshold/Step Function\n",
    "\n",
    "This is a commonly used activation function. As depicted in the diagram, it gives an output of 1 if the input is either 0 or positive. If the input is negative, it gives an output of 0. Expressed mathematically:\n",
    "\n",
    "\\[\n",
    "y_{\\text{out}} = f(y_{\\text{sum}}) = \n",
    "\\begin{cases}\n",
    "1, & \\text{if } x \\geq 0 \\\\\n",
    "0, & \\text{if } x < 0\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "The threshold function is almost like the step function, with the only difference being that \\( \\theta \\) is used as a threshold value instead of 0. Expressed mathematically:\n",
    "\n",
    "\\[\n",
    "y_{\\text{out}} = f(y_{\\text{sum}}) = \n",
    "\\begin{cases}\n",
    "1, & \\text{if } x \\geq \\theta \\\\\n",
    "0, & \\text{if } x < \\theta\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "#### C. ReLU (Rectified Linear Unit) Function\n",
    "\n",
    "ReLU is the most popular activation function used in convolutional neural networks and deep learning. It is of the form:\n",
    "\n",
    "\\[\n",
    "f(x) = \n",
    "\\begin{cases}\n",
    "x, & \\text{if } x \\geq 0 \\\\\n",
    "0, & \\text{if } x < 0\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "This means that \\( f(x) \\) is zero when \\( x \\) is less than zero and \\( f(x) \\) is equal to \\( x \\) when \\( x \\) is greater than or equal to zero. This function is differentiable, except at the point \\( x = 0 \\). The derivative of a ReLU is known as a sub-derivative.\n",
    "\n",
    "#### D. Sigmoid Function\n",
    "\n",
    "The sigmoid function is one of the most commonly used activation functions in neural networks. The need for the sigmoid function arises from the fact that many learning algorithms require the activation function to be differentiable and continuous. There are two types of sigmoid functions:\n",
    "\n",
    "1. **Binary Sigmoid Function**: \n",
    "\n",
    "\\[\n",
    "y_{\\text{out}} = f(x) = \\frac{1}{1 + e^{-kx}}\n",
    "\\]\n",
    "\n",
    "where \\( k \\) is the steepness or slope parameter. By varying the value of \\( k \\), sigmoid functions with different slopes can be obtained. It has a range of (0,1). The slope at the origin is \\( k/4 \\). As the value of \\( k \\) becomes very large, the sigmoid function approaches a threshold function.\n",
    "\n",
    "2. **Bipolar Sigmoid Function**:\n",
    "\n",
    "\\[\n",
    "y_{\\text{out}} = f(x) = \\frac{1 - e^{-kx}}{1 + e^{-kx}}\n",
    "\\]\n",
    "\n",
    "The range of values of sigmoid functions can be varied depending on the application. However, the range of (-1, +1) is most commonly adopted.\n",
    "\n",
    "#### E. Hyperbolic Tangent Function\n",
    "\n",
    "The hyperbolic tangent function is bipolar in nature and is widely used in a special type of neural network known as a Backpropagation Network. The hyperbolic tangent function is of the form:\n",
    "\n",
    "\\[\n",
    "y_{\\text{out}} = f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "\\]\n",
    "\n",
    "This function is similar to the bipolar sigmoid function.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Activation functions play a crucial role in shaping the outputs of artificial neurons, allowing neural networks to perform complex computations. The choice of activation function can significantly impact the performance and behavior of a neural network.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
