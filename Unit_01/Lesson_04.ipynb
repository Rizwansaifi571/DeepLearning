{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Deep Neural net with forward and back propagation from scratch – Python__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Date :__ 19, July, 2024. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a deep neural network containing a hidden layer with four units and one output layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Steps will be :`\n",
    "\n",
    "1. Visualizing the input data       \n",
    "\n",
    "2. Deciding the shapes of Weight and bias matrix        \n",
    "\n",
    "3. Initializing matrix, function to be used\n",
    "\n",
    "4. Implementing the forward propagation method\n",
    "\n",
    "5. Implementing the cost calculation\n",
    "\n",
    "6. Backpropagation and optimizing\n",
    "\n",
    "7. prediction and visualizing the output            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weights and Biases Initialization**:\n",
    "   - **Weights**: These are the parameters of the neural network that get adjusted during training. They connect the neurons between layers and are initially set to small random values to ensure that neurons start with different values.\n",
    "   - **Biases**: These are additional parameters added to the neurons to allow the model to fit the data better. They are usually initialized to zero.\n",
    "\n",
    "**Forward Propagation**:\n",
    "   - This is the process of calculating the output of the neural network by passing the input data through each layer.\n",
    "\n",
    "**Activation Functions**:\n",
    "   - These functions introduce non-linearity into the model, allowing it to learn more complex patterns. In this example, we use the **tanh** function for the hidden layer and the **sigmoid** function for the output layer.\n",
    "   - **tanh**: A function that squashes input values to be between -1 and 1.\n",
    "   - **sigmoid**: A function that squashes input values to be between 0 and 1.\n",
    "\n",
    "**Prediction**:\n",
    "   - After forward propagation, the final output (prediction) is obtained. If the output value is greater than 0.5, it predicts 1; otherwise, it predicts 0.\n",
    "\n",
    "Here’s how it all fits together:\n",
    "\n",
    "1. **Initialize Weights and Biases**:\n",
    "   - Randomly set weights to small values.\n",
    "   - Set biases to zero.\n",
    "\n",
    "2. **Forward Propagation**:\n",
    "```bash\n",
    "   - For the first layer, compute                           ( z^{[1]} = W^{[1]} x + b^{[1]} ).\n",
    "   - Apply the tanh activation to get                       ( a^{[1]} = \\tanh(z^{[1]}) ).\n",
    "   - For the second layer, compute                          ( z^{[2]} = W^{[2]} a^{[1]} + b^{[2]} ).\n",
    "   - Apply the sigmoid activation to get                    ( {^y} = a^{[2]} = σ(z^{[2]}) ).\n",
    "```\n",
    "3. **Make Predictions**:\n",
    "   - If \\( a^{[2]} > 0.5 \\), predict 1; otherwise, predict 0.\n",
    "\n",
    "This process allows the neural network to learn and make predictions based on the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Tanh in detail:`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tanh (hyperbolic tangent) function is an activation function used in neural networks to introduce non-linearity into the model. It transforms input values into an output range between -1 and 1. This helps the network learn complex patterns in the data.\n",
    "\n",
    "### Key Points about tanh:\n",
    "\n",
    "1. **Definition**:\n",
    "   - The tanh function is mathematically defined as:\n",
    "     ```bash\n",
    "     tanh(x) = e^x - e^{-x} / e^x + e^{-x}\n",
    "     ```\n",
    "   - Here, e is the base of the natural logarithm.\n",
    "\n",
    "2. **Properties**:\n",
    "   - **Range**: The output of tanh lies between -1 and 1.\n",
    "   - **Shape**: It has an S-shaped (sigmoid-like) curve but is symmetric around the origin (zero-centered).\n",
    "   - **Gradient**: The derivative of tanh is:\n",
    "     ```bash\n",
    "     {d}/{dx} tanh(x) = 1 - tanh^2 (x)\n",
    "     ```\n",
    "     This means that the gradients are smaller when the input is large in magnitude, which helps mitigate the vanishing gradient problem to some extent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Cost Function (or loss Function)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function measures how well our model's predictions match the actual outcomes. The goal is to minimize this cost to improve the model's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Use : `Binary Cross-Entropy Cost Function:`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This cost function is used when the output is a probability, like in logistic regression, where we predict values between 0 and 1.\n",
    "\n",
    "- `Formula :`\n",
    "    * __L=−(Y⋅log(Y_pred)+(1−Y)⋅log(1−Y_pred))__\n",
    "\n",
    "- `Explanation:`\n",
    "    * Y: The actual label (0 or 1).\n",
    "\n",
    "    * Y_pred: The predicted probability from the model (a value between 0 and 1).\n",
    "\n",
    "    * Cost is lower when prediction probability is closer to the actual label, and          \n",
    "    cost is high when prediction probability is far from the actual label."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
